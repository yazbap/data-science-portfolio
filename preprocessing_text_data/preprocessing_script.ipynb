{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4ef16ca",
   "metadata": {},
   "source": [
    "# Exploring Preprocessing Text Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "421f231a",
   "metadata": {
    "chartConfig": {
     "bar": {
      "hasRoundedCorners": true,
      "stacked": false
     },
     "type": "bar",
     "version": "v1"
    },
    "executionTime": 526,
    "lastSuccessfullyExecutedCode": "# Setup\nimport pandas as pd\nfrom sklearn.feature_extraction.text import TfidfVectorizer",
    "visualizeDataframe": false
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a2a8a9d",
   "metadata": {},
   "source": [
    "Preprocessing text data means that you are transforming text so it can be in a useable format for training. In this case we are using the `volunteer` data set which includes ads for volunteer opportunities in NYC. In this exercise we will prepare the `title` column for processing by filtering a sparse matrix to its most important columns.\n",
    "\n",
    "First I will show the dataset to gain familiarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b190af5d",
   "metadata": {
    "executionTime": 1015,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "volunteer = pd.read_csv('datasets/volunteer_opportunities.csv')\nvolunteer"
   },
   "outputs": [],
   "source": [
    "volunteer = pd.read_csv('datasets/volunteer_opportunities.csv')\n",
    "volunteer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8eda206",
   "metadata": {},
   "source": [
    "Then, I'll select the `title` column, and instantiate the Tfidf Vectorizer which is used to identify the weight of each word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed2da9f",
   "metadata": {
    "executionTime": 15,
    "lastSuccessfullyExecutedCode": "title_text = volunteer[\"title\"]\n\ntfidf = TfidfVectorizer()\ntext_tfidf = tfidf.fit_transform(title_text)"
   },
   "outputs": [],
   "source": [
    "title_text = volunteer[\"title\"]\n",
    "\n",
    "tfidf = TfidfVectorizer()\n",
    "text_tfidf = tfidf.fit_transform(title_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2a7ae1d",
   "metadata": {},
   "source": [
    "To get a better understanding of what the Tfidf matrix looks like, we convert `text_tfidf` into a data frame utilizing the method `.get_feature_names()` which outputs a list of words in the order they appear by column in the vectorizer. In turn, the df outputs a row for each row in our data set. Each column holds the weight a word has in relation to the other words in that row and all the other words from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660420c4",
   "metadata": {
    "executionTime": 68,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "feature_names = tfidf.get_feature_names()\ndf = pd.DataFrame(text_tfidf.toarray(), columns=feature_names)\nprint(df.head())"
   },
   "outputs": [],
   "source": [
    "feature_names = tfidf.get_feature_names()\n",
    "df = pd.DataFrame(text_tfidf.toarray(), columns=feature_names)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd77d698",
   "metadata": {},
   "source": [
    "Next, we want to filter the columns in `text_tfidf` to only include those that are the most relevant for processing later in the model. The function `return_weights` returns the top n words that weighed the most heavily in a particular row. We will use this function iteratively later on so we can get the most weighted words for the whole data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4de199db",
   "metadata": {
    "executionTime": 49,
    "lastSuccessfullyExecutedCode": "vocab = {v:k for k,v in tfidf.vocabulary_.items()}\n\ndef return_weights(vocab, original_vocab, vector, vector_index, top_n):\n    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n    \n    # Transform that zipped dict into a series\n    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n    \n    # Sort the series to pull out the top n weighted words\n    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n    return [original_vocab[i] for i in zipped_index]\n\n# Print out the weighted words\nprint(return_weights(vocab, tfidf.vocabulary_, text_tfidf, 1, 3))"
   },
   "outputs": [],
   "source": [
    "vocab = {v:k for k,v in tfidf.vocabulary_.items()}\n",
    "\n",
    "def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "    # Transform that zipped dict into a series which makes the data easier to operate on\n",
    "    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "    # Sort the series to pull out the top n weighted words\n",
    "    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "    return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# Print out the weighted words\n",
    "print(return_weights(vocab, tfidf.vocabulary_, text_tfidf, 1, 3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "516fffa2",
   "metadata": {},
   "source": [
    "To break down the function above, there are some key things to know. First, the `.vocabulary_` method outputs a dictionary in which all words from the dataset are the dictionary keys and the respective values are the column positions of each word in the tfidf matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce4ea8ea",
   "metadata": {
    "executionTime": 36,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "vocab_dict = tfidf.vocabulary_\nvocab_dict"
   },
   "outputs": [],
   "source": [
    "vocab_dict = tfidf.vocabulary_\n",
    "vocab_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42a8541",
   "metadata": {},
   "source": [
    "Therefore, the `vocab` variable just flips these values in the dict. Both of these are used to return what is needed.\n",
    "\n",
    "To understand `zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))`, I should break down `.indices` and `.data`. `.indices` holds all the column positions for each word in `text_tfidf`. `.data` holds the value for the relevance of the word. If you cross reference the output shown below with what was output from `.vocabulary_`, you'll see that for the second row, the column position of \"designer\" is 297 and the column position for \"web\" is 1095 which is aligned with what we see in row 2 of the data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1a37af",
   "metadata": {
    "executionTime": 48,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "print(text_tfidf[1].indices)\nprint(text_tfidf[1].data)\n"
   },
   "outputs": [],
   "source": [
    "print(text_tfidf[1].indices)\n",
    "print(text_tfidf[1].data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b909951",
   "metadata": {},
   "source": [
    "When you zip these together for a particular row, `zip` aggregates them into a tuple. Adding the function `dict` makes the column position of the word the key and the weight of that word the value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d180130",
   "metadata": {
    "executionTime": 12,
    "lastSuccessfullyExecutedCode": "zipped = zip(text_tfidf[1].indices, text_tfidf[1].data)\nprint(list(zipped))"
   },
   "outputs": [],
   "source": [
    "zipped = zip(text_tfidf[1].indices, text_tfidf[1].data)\n",
    "print(list(zipped))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2040e9b",
   "metadata": {},
   "source": [
    "Finally, we will filter the matrix with `words_to_filter` which now only has 1061 columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "071cc2d0",
   "metadata": {
    "executionTime": 355,
    "lastSuccessfullyExecutedCode": "def words_to_filter(vocab, original_vocab, vector, top_n):\n    filter_list = []\n    for i in range(0, vector.shape[0]):\n    \n        # Call the return_weights function and extend filter_list\n        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n        filter_list.extend(filtered)\n        \n    # Return the list in a set, so we don't get duplicate word indices\n    return set(filter_list)\n\n# Call the function to get the list of word indices\nfiltered_words = words_to_filter(vocab, tfidf.vocabulary_, text_tfidf, 3)\n\n# Filter the columns in text_tfidf to only those in filtered_words\nfiltered_text = text_tfidf[:, list(filtered_words)]"
   },
   "outputs": [],
   "source": [
    "def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "    filter_list = []\n",
    "    for i in range(0, vector.shape[0]):\n",
    "    \n",
    "        # Call the return_weights function and extend filter_list\n",
    "        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "        filter_list.extend(filtered)\n",
    "        \n",
    "    # Return the list in a set, so we don't get duplicate word indices\n",
    "    return set(filter_list)\n",
    "\n",
    "# Call the function to get the list of word indices\n",
    "filtered_words = words_to_filter(vocab, tfidf.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# Filter the columns in text_tfidf to only those in filtered_words\n",
    "filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c92798f",
   "metadata": {
    "executionTime": 17,
    "jupyter": {
     "outputs_hidden": false,
     "source_hidden": false
    },
    "lastSuccessfullyExecutedCode": "filtered_text.shape"
   },
   "outputs": [],
   "source": [
    "filtered_text.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b2a51b0",
   "metadata": {},
   "source": [
    "Of course, preprocessing will look different based on the model you end up using as well as the other data available. This example was to explain the steps that could be involved with preprocessing text data."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Welcome to DataCamp Workspaces.ipynb",
   "provenance": []
  },
  "editor": "DataCamp Workspace",
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
